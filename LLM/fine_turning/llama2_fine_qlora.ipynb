{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvf_1B8q9g49"
      },
      "outputs": [],
      "source": [
        "# Googleドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjM_ZQ8J9jd_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.makedirs(\"/content/drive/My Drive/work\", exist_ok=True)\n",
        "%cd \"/content/drive/My Drive/work\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp3lgJNM9luq"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/artidoro/qlora\n",
        "%cd qlora\n",
        "!pip install -U -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAUuZvFG9lz5"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVX04chu9l69"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFgGmNx0iiQ7"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpuOIg9Glocp"
      },
      "outputs": [],
      "source": [
        "%cd llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaB5fii2jMxq"
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-O98UAXCN08"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh0GBv8RlgEt"
      },
      "outputs": [],
      "source": [
        "%cd qlora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "dataset_name=\"./test.json\"\n",
        "\n",
        "!python qlora.py \\\n",
        "    --model_name $model_name \\\n",
        "    --output_dir ./output/test\\\n",
        "    --dataset $dataset_name \\\n",
        "    --dataset_format input-output\\\n",
        "    --max_steps 1000 \\\n",
        "    --use_auth \\\n",
        "    --logging_steps 10 \\\n",
        "    --save_strategy steps \\\n",
        "    --data_seed 42 \\\n",
        "    --save_steps 500 \\\n",
        "    --save_total_limit 40 \\\n",
        "    --dataloader_num_workers 1 \\\n",
        "    --group_by_length \\\n",
        "    --logging_strategy steps \\\n",
        "    --remove_unused_columns False \\\n",
        "    --do_train \\\n",
        "    --lora_r 64 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_modules all \\\n",
        "    --double_quant \\\n",
        "    --quant_type nf4 \\\n",
        "    --bits 4 \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type constant \\\n",
        "    --gradient_checkpointing \\\n",
        "    --source_max_len 16 \\\n",
        "    --target_max_len 4096 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --eval_steps 187 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --adam_beta2 0.999 \\\n",
        "    --max_grad_norm 0.3 \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --weight_decay 0.0 \\\n",
        "    --seed 0 \\\n",
        "    --load_in_4bit \\\n",
        "    --use_peft \\\n",
        "    --batch_size 4 \\\n",
        "    --gradient_accumulation_steps 2 \\\n",
        "    -- fp16"
      ],
      "metadata": {
        "id": "i0GbXXbSWqDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHVIMjg-kItM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "#load base model\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
        "\n",
        "\n",
        "#LoRAのパスを指定\n",
        "peft_name = \"./output/test/checkpoint-1000/adapter_model\"\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    peft_name,\n",
        "    device_map={\"\":0}\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "device = \"cuda:0\"\n",
        "def ask(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "  print(outputs)\n",
        "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "text = \"OpenCVってなに？\"\n",
        "\n",
        "ask(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7yavGEJV7jW"
      },
      "outputs": [],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llama.cpp\n"
      ],
      "metadata": {
        "id": "1tLjn5ebPW7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "1BWsfB8A2p8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "id": "KXOFQXxM2uOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make -j && ./main -m ./llama-v2.gguf  -p \"Give me the code to get a random number in python.\" -n 400 -e"
      ],
      "metadata": {
        "id": "6LiyoiQC2uQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QLora llama"
      ],
      "metadata": {
        "id": "hEf5xDw2PcBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 convert-lora-to-ggml.py ./output/test/checkpoint-1000/adapter_model"
      ],
      "metadata": {
        "id": "Lxap0tjX2uSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m ./llama-v2.gguf --lora./qlora/output/test/checkpoint-1000/adapter_model/ggml-adapter-model.bin -p \"\" -n 400 -e\n"
      ],
      "metadata": {
        "id": "q4TQ1b4r3VbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXVhFLLgo4UT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a409872-1bfb-42a1-9e53-14fe2ab96555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/model_planet_cloud/src/businesslogic\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/model_planet_cloud/src/businesslogic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "693555fa-d5b9-4172-9baf-ea804a740197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/model_planet_cloud/modules/blip/weights\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/model_planet_cloud/modules/blip/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e8305e-f058-41b4-9d91-b78a808e1f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-12 11:54:22--  https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n",
      "Resolving www-proxy.ao.ericsson.se (www-proxy.ao.ericsson.se)... 150.236.2.23\n",
      "Connecting to www-proxy.ao.ericsson.se (www-proxy.ao.ericsson.se)|150.236.2.23|:8080... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 1908759703 (1.8G) [application/octet-stream]\n",
      "Saving to: ‘model_base_retrieval_coco.pth’\n",
      "\n",
      "model_base_retrieva 100%[===================>]   1.78G  21.8MB/s    in 86s     \n",
      "\n",
      "2023-09-12 11:55:49 (21.2 MB/s) - ‘model_base_retrieval_coco.pth’ saved [1908759703/1908759703]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41332dc2-9d76-4449-8f5a-a343bbad2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/model_planet_cloud/src/businesslogic\n"
     ]
    }
   ],
   "source": [
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193f4172-e96e-40c6-a447-d1c5f6cbc1cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 117 layers, 4736733 parameters, 0 gradients, 10.8 GFLOPs\n",
      "load checkpoint from /mnt/model_planet_cloud/modules/blip/weights/model_base_retrieval_coco.pth\n",
      "model paht:  /mnt/model_planet_cloud/modules/blip/weights/model_base_retrieval_coco.pth\n",
      "file_names 01.jpg\n",
      "response {'bboxes': [[171, 89, 272, 141], [99, 39, 178, 59], [178, 130, 196, 143], [139, 120, 159, 130], [236, 78, 250, 99], [33, 83, 211, 170], [265, 79, 274, 136], [184, 0, 202, 10], [223, 79, 231, 92], [163, 119, 199, 128], [102, 22, 169, 44], [197, 117, 208, 126], [218, 120, 235, 140], [50, 122, 69, 153], [4, 63, 62, 73], [116, 131, 143, 170], [111, 117, 116, 122], [128, 103, 152, 106], [7, 96, 20, 117], [0, 95, 9, 117], [181, 147, 205, 162], [263, 124, 270, 130], [122, 2, 144, 19], [178, 54, 235, 64]], 'cls_score_list': [0.9306171536445618, 0.769067108631134, 0.7662612199783325, 0.7617154717445374, 0.7541907429695129, 0.7417063117027283, 0.7021245360374451, 0.6944525837898254, 0.6944412589073181, 0.6783560514450073, 0.6705018877983093, 0.6326576471328735, 0.599128782749176, 0.592547595500946, 0.5868123173713684, 0.5849562287330627, 0.5691843032836914, 0.5683683156967163, 0.5482496023178101, 0.5457543730735779, 0.5415230393409729, 0.5404179692268372, 0.5169838070869446, 0.5143950581550598], 'class_id_list': [206, 28, 641, 548, 792, 1122, 206, 1111, 792, 516, 958, 548, 1177, 1177, 28, 1177, 113, 1185, 702, 702, 1177, 641, 958, 958], 'input_img_shape': [183, 275, 3], 'cls_name_list': ['car_(automobile)', 'awning', 'license_plate', 'headlight', 'person', 'truck', 'car_(automobile)', 'traffic_light', 'person', 'grill', 'signboard', 'headlight', 'wheel', 'wheel', 'awning', 'wheel', 'blinker', 'windshield_wiper', 'motorcycle', 'motorcycle', 'wheel', 'license_plate', 'signboard', 'signboard']}\n",
      "device cpu\n",
      "The image and text is matched with a probability of 0.2168\n",
      "The image and text is matched with a probability of 0.0022\n",
      "The image and text is matched with a probability of 0.1794\n",
      "The image and text is matched with a probability of 0.0013\n",
      "The image and text is matched with a probability of 0.0080\n",
      "Inference Result: This car is black\n",
      "device cpu\n",
      "The image and text is matched with a probability of 0.1508\n",
      "The image and text is matched with a probability of 0.1298\n",
      "The image and text is matched with a probability of 0.3843\n",
      "The image and text is matched with a probability of 0.6212\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"inference.py\", line 239, in <module>\n",
      "    res_json = infer.inference(img, None, file_names)\n",
      "  File \"inference.py\", line 177, in inference\n",
      "    pipeline.classify_blip(imgs)\n",
      "  File \"/mnt/model_planet_cloud/src/utils/pipeline.py\", line 890, in classify_blip\n",
      "    itm_output = self.model_blip(img,caption,match_head='itm')\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/model_planet_cloud/src/blip/models/blip_itm.py\", line 52, in forward\n",
      "    output = self.text_encoder(text.input_ids,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/model_planet_cloud/src/blip/models/med.py\", line 781, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/model_planet_cloud/src/blip/models/med.py\", line 445, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/model_planet_cloud/src/blip/models/med.py\", line 361, in forward\n",
      "    cross_attention_outputs = self.crossattention(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/model_planet_cloud/src/blip/models/med.py\", line 286, in forward\n",
      "    attention_output = self.output(self_outputs[0], hidden_states)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/model_planet_cloud/src/blip/models/med.py\", line 236, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/functional.py\", line 1848, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d7d3a6-0bba-46d3-b95e-66068cc6fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01cbfaa3-e9f9-4b15-ac50-ac93ece9eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-lightning        1.5.10\n",
      "torch                    1.7.0+cu101\n",
      "torchaudio               0.9.0\n",
      "torchmetrics             1.1.1\n",
      "torchvision              0.8.1+cu101\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587dbbd-acb2-4b6a-bc9a-dd15c18a2552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
